{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCFG Simulator tutorial\n",
    "\n",
    "7/14/19\n",
    "\n",
    "Language and Cognitive Architecture Lab\n",
    "Sean Anderson, Research Assistant\n",
    "\n",
    "A quick tutorial for pcfg.py, a PCFG simulator that reads in a custom PCFG and generates sentences with corresponding Information metrics.\n",
    "\n",
    "These metrics are supported:\n",
    "\n",
    "* Surprisal (lexical)\n",
    "* Lexical Entropy (over words)\n",
    "* Structural Entropy (over meanings)\n",
    "* Lexical Entropy Reduction\n",
    "* Structural Entropy Reduction\n",
    "* Kulback-Liebler Divergence, both (Pk+1 || Pk) and (Pk || Pk+1) (structural)\n",
    "* Mutual Information (structural)\n",
    "\n",
    "For formulas, justification, and discussion about some of these metrics, please see Hale (2001), Hale (2016), and Levy (2008).\n",
    "\n",
    "Before starting, you need the following in your working directory:\n",
    "\n",
    "```\n",
    "pcfg.py\n",
    "artificial_grammar.py\n",
    "corpus.py\n",
    "YOUR_CUSTOM_PCFG.txt\n",
    "```\n",
    "\n",
    "This program uses a Corpus of all legal sentences from the grammar provided. Generating this might take a while, but if you'd like to save a copy of the Corpus for future use you should have ```pickle``` installed. ```pickle``` is a Python module that writes and reads large Python objects to/from a file. Learn more [here](https://docs.python.org/3/library/pickle.html \"Python3 Pickle docs\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the custom grammar\n",
    "\n",
    "Custom grammars are defined in a text file. \n",
    "Define your grammer as a bunch of rules, one per line, in the following format:\n",
    "\n",
    "```P(rule) NonterminalSymbol -> Symbol1 Symbol2 Symbol3 ...```\n",
    "\n",
    "Where 'NonterminalSymbol' is the parent symbol where this rule can be applied, and P(rule) is the probability that this rule is applied on the parent symbol.\n",
    "\n",
    "There are a number of restrictions enforced on the grammar, including that only one recursive rule is allowed, and that the probabilites of rules using the same parent symbol must sum to 1.\n",
    "\n",
    "Below is an example of a recursive PCFG:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# phrase rules\n",
    "0.5 CP -> CP R S\n",
    "0.5 CP -> S R S\n",
    "\n",
    "# shape rules\n",
    "0.5 S -> Circle\n",
    "0.2 S -> Diamond\n",
    "0.2 S -> Square\n",
    "0.1 S -> Triangle\n",
    "\n",
    "# relation rules\n",
    "0.25 R -> LeftOf\n",
    "0.25 R -> RightOf\n",
    "0.25 R -> Above\n",
    "0.25 R -> Below\n",
    "\n",
    "# use '#' to make a comment line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCFG is implemented using artificial_grammar.py.\n",
    "\n",
    "## quickly, how it works\n",
    "\n",
    "The program reads in the custom grammar defined in the text file. Then it generates a Corpus of all possible legal sentences using the grammar, up to a certain recursion depth if specified. Using this Corpus, the program can calculate each of the complexity metrics at each word (i.e. timestep) in a sentence. \n",
    "\n",
    "Since generating all the sentences can be time and memory intensive, the Corpus is saved as a ```pickle``` file in the current directory. If ```pcfg.py``` is run with the the ```-p``` or ```--pickle``` flag, the program searches for an existing pickle file (.corpus). If there's one with the name as the grammar file, that corpus is used (skipping generating a new one).\n",
    "\n",
    "## generating a sentence\n",
    "\n",
    "To generate a single sentence, run with the --single/-s flag:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 pcfg.py PCFG_flat.txt CP --single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules\n",
      "=====\n",
      "parent: CP\n",
      "distr: [0.5, 0.5]\n",
      "exp's: [('S', 'R', 'S'), ('Triangle', 'LeftOf', 'Circle')]\n",
      "\n",
      "parent: S\n",
      "distr: [0.5, 0.5]\n",
      "exp's: [('Diamond',), ('Square',)]\n",
      "\n",
      "parent: R\n",
      "distr: [0.333, 0.333, 0.333]\n",
      "exp's: [('Above',), ('Below',), ('RightOf',)]\n",
      "\n",
      "\n",
      "metric    word  surprisal  entropy_lex  entropy_struct    ER_lex  ER_struct  \\\n",
      "i                                                                             \n",
      "0       Square   2.000722     1.499750        2.791584  0.000000   0.206622   \n",
      "1        Below   1.584963     1.584963        2.584963  0.584963   1.584963   \n",
      "2       Square   1.000000     1.000000        1.000000  1.000000   1.000000   \n",
      "\n",
      "metric  KL_diverge_PQ  KL_diverge_QP  \n",
      "i                                     \n",
      "0            2.000722            inf  \n",
      "1            1.584963            inf  \n",
      "2            1.000000            inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCFG_generator/corpus.py:534: RuntimeWarning: divide by zero encountered in log2\n",
      "  divergence += (2 ** logp) * (logp - np.log2(actualq))\n"
     ]
    }
   ],
   "source": [
    "%run pcfg.py PCFG_flat.txt CP --single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: every once in a while, numpy.log2 results in a RuntimeWarning when dividing by zero.  -np.inf is returned in these situtations._\n",
    "\n",
    "_Another Note: because nplog2(0) = np.inf, KL_diverge_QP always evaulates to np.inf. This part needs to be fixed._\n",
    "\n",
    "_Note 3: sentences are selected uniformly from corpus, not from their likelihood given the PCFG._\n",
    "\n",
    "The PCFG_flat.txt was read, and the rules are printed. A .corpus pickle save was generated (```PCFG_flat.corpus```), as well as a csv storing the data from the sentence (```sentence.csv```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCFG Simulator Tutorial.ipynb  corpus.py\r\n",
      "PCFG_example.txt               pcfg.py\r\n",
      "PCFG_flat.corpus               sentence.csv\r\n",
      "PCFG_flat.txt                  test.py\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/                   test.py~\r\n",
      "artificial_grammar.py\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage message for the program should also be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: pcfg.py [-h] [-l LIMIT_DEPTH] [-p | -g] [-s]\n",
      "               grammar_file initial_symbol\n",
      "\n",
      "Simulate sentence generation from a Custom PCFG,with Information Theoretic\n",
      "Metrics\n",
      "\n",
      "positional arguments:\n",
      "  grammar_file          custom grammar.txt file\n",
      "  initial_symbol        symbol used to start generation\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -l LIMIT_DEPTH, --limit_depth LIMIT_DEPTH\n",
      "                        set depth limit for recursive generation\n",
      "  -p, --pickle          use already existing Corpus pickle file\n",
      "  -g, --generate        force generation of Corpus using grammar\n",
      "  -s, --single          generate only one sentence\n"
     ]
    }
   ],
   "source": [
    "%run pcfg.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sentence.csv``` stores the same data printed when running the program. It is important to note that the first row of the table represents the metrics _before_ reading the first word. For example, with a context of 0 words, I have a Surprisal of ~2 bits when I see the first word is _Square_ (given PCFG_flat). My Lexical Entropy before reading the word _Square_ is ~1.5 bits, and my Lexical Entropy Reduction from seeing no words to having read _Square_ is 0. This follows similarly for Structural Entropy and Entropy Reduction.\n",
    "\n",
    "Importantly, this means that in the last row the Surprisal column displays the Surprisal upon reading the last word in the sentence, while the Lexical Entropy displays the entropy right before reading the last word.\n",
    "\n",
    "## caveats\n",
    "\n",
    "As the program is currently designed, if the grammar is altered but the name is kept, then generating the corpus will overwrite the previous corpus (unless ```--pickle``` is passed). If ```--pickle``` is passed, then the pickle .corpus file made with the old grammar will be used, not the new grammar. The program does not effectively keep track of which corpus comes from which grammar. Similarly, every time ```--single``` is passed, sentence.csv will be overwritten with the new sentence.\n",
    "\n",
    "## coming soon\n",
    "\n",
    "* Generation without ```--single``` flag: I'm still working on a way to have a csv effectively contain the stats for all the sentences of the corpus, with their likelihoods given the grammar. This would enable generation in one go. \n",
    "* Mutual Information calculation\n",
    "* For ambiguity and meaning, see below.\n",
    "* More efficient sentence generation. Right now stacks and sentence-in-makings are duplicated across stack frames, wasting lots of memory. I'm trying to figure out a way to not have that happen, and make generating the corpus faster.\n",
    "\n",
    "## how pcfg.py handles ambiguous grammars\n",
    "\n",
    "Every time a sentence is generated using the grammar, its likelihood is stored. As ambiguous grammars can generate the same sentence in different ways, we might end up with 2 or more likelihoods for a single sentence. In the corpus, each of these likelihoods are stored as a 'SyntaxTree' in a list linked to the sentence. Any sentence that has more than one SyntaxTree (effectively, likelihood) linked to it is treated as ambiguous. Structural Entropy and Lexical Entropy are then calculated accordingly, the former by summing over each possible meaning (i.e. SyntaxTree) and the latter by each sentence (all likelihoods pooled into one).\n",
    "\n",
    "Right now, SyntaxTree only stores the likelihood of one possible generation of the sentence, but the program could easily be modified to store more things about the meaning of the sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
